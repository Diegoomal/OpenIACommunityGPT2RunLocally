{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Needed libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3> Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "result = generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but I'm not afraid of language models. Because the language model tells you, this isn't your normal,\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, I\\'m my own language,\" he said. \"But you\\'re getting into questions when you come up with new'},\n",
       " {'generated_text': \"Hello, I'm a language model, so yes, this would be pretty nice. Thanks.\\n\\nI hope I'm not the only one who\"},\n",
       " {'generated_text': \"Hello, I'm a language model, so I think I'm gonna have to do something better.\\n\\n\\nI got the idea for this after my\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I'm an expert to many of our users, we're experts to many of our colleagues. I'm the\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3> Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiment-analysis - Classificação de Texto:\n",
    "\n",
    "Este pipeline classifica o sentimento de um texto como positivo, negativo ou neutro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier(\"This movie is great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9983599781990051}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3> Fill Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preenchimento de Máscara ('fill-mask'):\n",
    "\n",
    "Este pipeline preenche lacunas em um texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill_mask = pipeline('fill-mask', model='gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = fill_mask(\"This is [MASK] a test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3> Question Answering (QA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question-answering - Pergunta e Resposta:\n",
    "\n",
    "Este pipeline responde a perguntas com base em um contexto fornecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at gpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "question_answerer = pipeline('question-answering', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = question_answerer(context=\"Albert Einstein was a German physicist.\", question=\"Who was Albert Einstein?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.10965800285339355,\n",
       " 'start': 0,\n",
       " 'end': 39,\n",
       " 'answer': 'Albert Einstein was a German physicist.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3> Reconhecimento de Entidade Nomeada (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER - Reconhecimento de Entidade Nomeada:\n",
    "\n",
    "Este pipeline identifica e classifica entidades nomeadas em um texto, como nomes de pessoas, organizações, locais, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline('ner', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ner(\"John Doe is a software engineer at Google.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_0',\n",
       "  'score': 0.76308954,\n",
       "  'index': 0,\n",
       "  'word': 'John',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9620186,\n",
       "  'index': 1,\n",
       "  'word': 'ĠDoe',\n",
       "  'start': 4,\n",
       "  'end': 8},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9991829,\n",
       "  'index': 2,\n",
       "  'word': 'Ġis',\n",
       "  'start': 8,\n",
       "  'end': 11},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99996233,\n",
       "  'index': 3,\n",
       "  'word': 'Ġa',\n",
       "  'start': 11,\n",
       "  'end': 13},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9942252,\n",
       "  'index': 4,\n",
       "  'word': 'Ġsoftware',\n",
       "  'start': 13,\n",
       "  'end': 22},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99918157,\n",
       "  'index': 5,\n",
       "  'word': 'Ġengineer',\n",
       "  'start': 22,\n",
       "  'end': 31},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99834085,\n",
       "  'index': 6,\n",
       "  'word': 'Ġat',\n",
       "  'start': 31,\n",
       "  'end': 34},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9449127,\n",
       "  'index': 7,\n",
       "  'word': 'ĠGoogle',\n",
       "  'start': 34,\n",
       "  'end': 41},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9945956,\n",
       "  'index': 8,\n",
       "  'word': '.',\n",
       "  'start': 41,\n",
       "  'end': 42}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3> Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summarization - Sumarização de Texto:\n",
    "\n",
    "Este pipeline gera uma versão resumida de um texto longo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'GPT2LMHeadModel' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 20, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/mal/miniconda3/envs/project-env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result = summarizer(\"The European Space Agency plans to establish a permanent base on the moon.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'The European Space Agency plans to establish a permanent base on the moon.\\n\\nThe European Space Agency'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3> Conversational AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# generator = pipeline('conversational', model='gpt2')\n",
    "\n",
    "# # Iniciar a conversa\n",
    "# history = []\n",
    "\n",
    "# for i in range(10):\n",
    "\n",
    "#     user_input = input(\"Você: \")\n",
    "#     history.append(user_input)\n",
    "\n",
    "#     # Obter resposta do modelo\n",
    "#     bot_response = generator(history)\n",
    "\n",
    "#     # Mostrar a resposta do modelo\n",
    "#     print(\"Bot:\", bot_response)\n",
    "\n",
    "#     # Adicionar resposta do bot ao histórico de conversação\n",
    "#     history.append(bot_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3> PyTorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2Model\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = GPT2Model.from_pretrained('gpt2')\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3> TensorFlow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, TFGPT2Model\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = TFGPT2Model.from_pretrained('gpt2')\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
